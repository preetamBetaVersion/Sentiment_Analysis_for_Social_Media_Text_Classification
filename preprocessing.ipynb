{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26873.53s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./env/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in ./env/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./env/lib/python3.9/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.9/site-packages (from nltk) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26879.69s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in ./env/lib/python3.9/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in ./env/lib/python3.9/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in ./env/lib/python3.9/site-packages (from nltk>=3.1->textblob) (8.1.3)\n",
      "Requirement already satisfied: joblib in ./env/lib/python3.9/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./env/lib/python3.9/site-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.9/site-packages (from nltk>=3.1->textblob) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "# module for spelling check\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/harekrishna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text data and save it into a pandas dataframe\n",
    "\n",
    "with open('raw_userdata.txt') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocessing:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def remove_newlines(self):\n",
    "        list_of_data_without_newline = []\n",
    "        for line in self.data:\n",
    "            line = re.sub(r'\\n','',line)\n",
    "            list_of_data_without_newline.append(line)\n",
    "        return list_of_data_without_newline\n",
    "    \n",
    "    def convert_text_to_lowercase(self):\n",
    "        lowercased_user_posts = []\n",
    "        for line in self.data:\n",
    "            lowercased_user_posts.append(line.lower())\n",
    "        return lowercased_user_posts\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        text_without_punctuation = []\n",
    "        for line in self.data:\n",
    "            line = re.sub(r'[^\\w\\s\\']', '',line)\n",
    "            text_without_punctuation.append(line)\n",
    "        return text_without_punctuation\n",
    "    \n",
    "    def remove_links(self):\n",
    "        user_data_without_links = []\n",
    "        for line in self.data:\n",
    "            url_pattern = r\"https?://\\S+\"\n",
    "            # remove links from texts\n",
    "            line = re.sub(url_pattern,'',line)\n",
    "            user_data_without_links.append(line)\n",
    "        return user_data_without_links\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        user_data_without_stopwords = []\n",
    "        for line in self.data:\n",
    "            word_tokens = line.split()\n",
    "            filtered_tokens = []\n",
    "            for word in word_tokens:\n",
    "                if word not in stopwords.words('english'):\n",
    "                    filtered_tokens.append(word)\n",
    "            filtered_text = \" \".join(filtered_tokens)\n",
    "            user_data_without_stopwords.append(filtered_text)\n",
    "        return user_data_without_stopwords\n",
    "    \n",
    "    def remove_numbers(self):\n",
    "        user_posts_without_numbers = []\n",
    "        for line in self.data:\n",
    "            filtered_text = re.sub(r'\\d','',line)\n",
    "            user_posts_without_numbers.append(filtered_text)\n",
    "        return user_posts_without_numbers\n",
    "    \n",
    "    def correct_spelling(self):\n",
    "        user_data_with_correct_spellings = []\n",
    "        for line in self.data:\n",
    "            blob = TextBlob(line)\n",
    "            corrected_line = str(blob.correct()).replace('TextBlob','').strip()\n",
    "            user_data_with_correct_spellings.append(corrected_line)\n",
    "        return user_data_with_correct_spellings\n",
    "    \n",
    "    def lemmatize(self):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_user_data = []\n",
    "        for line in self.data:\n",
    "            word_tokens = line.split()\n",
    "            filtered_tokens = []\n",
    "            for word in word_tokens:\n",
    "                word = lemmatizer.lemmatize(word, pos='v')\n",
    "                filtered_tokens.append(word)\n",
    "            filtered_text = \" \".join(filtered_tokens)\n",
    "            lemmatized_user_data.append(filtered_text)\n",
    "        return lemmatized_user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_newline = Data_preprocessing(data)\n",
    "\n",
    "# remove newlines from list text elements\n",
    "user_posts_without_newlines = remove_newline.remove_newlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "lower = Data_preprocessing(user_posts_without_newlines)\n",
    "\n",
    "# convert list text elements to lowercase\n",
    "lowercase_user_posts = lower.convert_text_to_lowercase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_punctuation = Data_preprocessing(lowercase_user_posts)\n",
    "\n",
    "# remove punctuation from list text elements\n",
    "user_posts_without_punctuation = remove_punctuation.remove_punctuation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_links = Data_preprocessing(user_posts_without_punctuation)\n",
    "\n",
    "# remove links from list text elements\n",
    "user_posts_without_links = remove_links.remove_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_stopwords = Data_preprocessing(user_posts_without_links)\n",
    "\n",
    "# remove stopwords from list text elements\n",
    "user_posts_without_stopwords = remove_stopwords.remove_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_numbers = Data_preprocessing(user_posts_without_stopwords)\n",
    "\n",
    "# remove numbers from list text elements\n",
    "user_posts_without_numbers = remove_numbers.remove_numbers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "remove_spelling_errors = Data_preprocessing(user_posts_without_numbers)\n",
    "\n",
    "# correct spellings if any errors exist\n",
    "user_posts_without_spelling_errors = remove_spelling_errors.correct_spelling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Data_preprocessing class\n",
    "lemmatize_user_data = Data_preprocessing(user_posts_without_spelling_errors)\n",
    "\n",
    "# sentences with lemmatized words\n",
    "lemmatize_user_data = lemmatize_user_data.lemmatize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean user data into a text file\n",
    "\n",
    "for line in user_posts_without_spelling_errors:\n",
    "    with open('preprocessed_user_data.txt','a') as f:\n",
    "        f.write(line+' ') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34ab44d21b0d28fca004963d1d04c7edca0610639bd25a7e24f1a59a85c62704"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
